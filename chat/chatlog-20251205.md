stprior: Ok, so earlier blocks can affect later blocks, not vice-versa. Can we look at the lowest block in isolation, so like T(c)(x) represents an operation where a prompted transformer acts on an embedding vector x, T_1(c)(x) represents the lowest block as an operation on x?

GitHub Copilot: Looking at your question about isolating the lowest block, let me first check the current notation and theory documentation to understand how we're representing block operations. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md#1-1), lines 1 to 100

Yes, absolutely! This is a natural way to think about it. Looking at the theory document, we have:

**Full transformer as composition of blocks:**
$$x_j^L = B_L \circ B_{L-1} \circ \cdots \circ B_1 (x_j^0)$$

So your notation $T_1(c)(x)$ for the lowest block makes sense. Here's how I'd frame it:

| Expression | Meaning |
|------------|---------|
| $T(c)(x)$ | Full prompted transformer acting on embedding $x$ |
| $T_i(c)(x)$ | Block $i$ of prompted transformer acting on residual $x$ |

**For the lowest block specifically:**

$$T_1(c)(x) = B_1(x)$$

where $B_1$ is the block operator from theory.md:
$$B_1(x) = x + \tilde{A}^1(x) + \tilde{M}^1(x + \tilde{A}^1(x))$$

**Why the lowest block is special:**

The attention in $T_1(c)$ only sees:
- The context embeddings $\{x_k^0\}_{k < j}$ (fixed once context is given)
- The input $x$ at position $j$

So $T_1(c)$ is genuinely a function just of $x$—the context provides fixed key/value vectors that don't depend on $x$.

**The composition view:**
$$T(c)(x) = T_L(c) \circ T_{L-1}(c) \circ \cdots \circ T_1(c)(x)$$

But note: for $i > 1$, $T_i(c)$ is more complex because it depends on what earlier blocks computed *at the context positions*. The causal structure means $T_1(c)$ is cleanest to analyze in isolation.

Shall I add this notation to notation.md?

stprior: I'm not yet sure if T_2(c) should represent the second block, or the first and second block combined, this may emerge as we work through examples. If we concentrate on the first block, can we break down the contribution at each position in terms of the three feature types? For example, if investigating why one token was predicted instead of another we could use these two unembedding vectors as a subspace to investigate from? We would need to look at attention calculation seperately.

GitHub Copilot: Great point about keeping the $T_i$ notation flexible for now. Let me think through the first block decomposition.

**First Block Contribution Decomposition**

For the first block acting on embedding $x$ at position $j$:

$$\Delta x_j^1 = \underbrace{\tilde{A}^1(x)}_{\text{attention}} + \underbrace{\tilde{M}^1(x + \tilde{A}^1(x))}_{\text{MLP}}$$

**Projecting onto a token subspace**

If we're investigating why token A was predicted over token B, we can define a 2D subspace spanned by $\{\overline{A}, \overline{B}\}$ and project the contributions:

$$\text{logit}(A) - \text{logit}(B) = \langle \overline{A} - \overline{B}, LN(x_j^L) \rangle$$

The "logit difference direction" $d = \overline{A} - \overline{B}$ is a single vector we can dot against any residual or contribution.

**Three feature types for block 1:**

At each position $k \leq j$, the residual $x_k^0$ entering block 1 decomposes as:

| Feature Type | Symbol | Description |
|--------------|--------|-------------|
| Token embedding | $\underline{t_k}$ | What token is at position $k$ |
| Positional embedding | $p_k$ | Where position $k$ is |
| Combined | $x_k^0 = \underline{t_k} + p_k$ | Full input to block 1 |

**Attention contribution breakdown:**

The attention output at position $j$ is:
$$\tilde{A}^1(x_j^0) = \sum_h \sum_{k \leq j} A^{1,h}_{jk} \cdot V^{1,h}(LN(x_k^0))$$

where the value computation is linear: $V^{1,h}(LN(x_k^0))$ depends on the features at position $k$.

**Separating "what" from "where":**

Since $x_k^0 = \underline{t_k} + p_k$ and the value projection is linear:
$$V^{1,h}(LN(x_k^0)) \approx V^{1,h}(LN(\underline{t_k})) + V^{1,h}(LN(p_k))$$

⚠️ **But layer norm breaks this!** $LN(a + b) \neq LN(a) + LN(b)$

So we can't cleanly separate token and position contributions through attention. We could:
1. Study empirically how much position vs token matters
2. Use the combined $x_k^0$ and accept they're entangled
3. Consider ablation experiments (zero out positional, etc.)

**For the logit difference investigation:**

We can compute:
$$\langle d, \Delta x_j^1 \rangle = \langle d, \tilde{A}^1(x_j^0) \rangle + \langle d, \tilde{M}^1(\cdot) \rangle$$

This tells us how much block 1's attention vs MLP pushed toward A vs B.

**Attention patterns separately:**

You're right that attention calculation (Q·K) is separate from value movement. The pattern $A^{1,h}_{jk}$ depends on:
- Query from position $j$: $Q^{1,h}(LN(x_j^0))$
- Keys from positions $k \leq j$: $K^{1,h}(LN(x_k^0))$

This determines *where* to look, while the value projection determines *what* gets moved.

Shall I add a section to the theory doc sketching out this "logit lens" / "logit difference" analysis framework for block 1?

stprior: I think most of this would go in speculation for now.

GitHub Copilot: Good call. Let me add this to speculation.md. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#1-1), lines 1 to 100

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#100-100), lines 100 to 136

Using "Replace String in File"


```

```
Added a new "Block-wise Analysis" section to speculation.md covering:
- $T_i(c)$ notation (leaving the composition question open)
- Why block 1 is cleanest to analyze in isolation
- Logit difference direction $d = \overline{A} - \overline{B}$ for token comparison
- The attention/MLP decomposition of $\Delta x_j^1$
- The layer norm entanglement problem for separating token vs position
- The "where" (Q·K) vs "what" (V) distinction in attention

Also added the $T_2(c)$ meaning as open question #4.

Made changes.

stprior: I've added a section on Functional Discourse Grammar. Can you review, and share any ideas how this should influnce the analysis? 

GitHub Copilot: Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md)

This is a fascinating addition! FDG provides a rich framework that could significantly inform the analysis. Here are my thoughts:

**Key Parallels to Explore:**

1. **Parallel multi-level processing** — FDG's insight that utterances are described at multiple levels *simultaneously* maps well onto the transformer's residual stream. Each position's residual $x_j^i$ may encode multiple linguistic-levels in superposition, not sequentially "first syntax, then semantics."

2. **Layered hierarchical structure** — FDG's head/modifier organization within each linguistic-layer resonates with attention patterns. Attention heads might learn to identify syntactic heads vs modifiers, or thematic roles at the representational level.

3. **The formulation/interpretation duality** — This is a key insight. In FDG, formulation proceeds "top-down" (intention → phonology). But LLMs do both:
   - *Interpretation*: Early transformer-layers parse the input (morphosyntactic → representational?)
   - *Formulation*: Later transformer-layers generate appropriate continuation (representational → morphosyntactic?)
   
   The "indistinct boundary" you note might be where the residual stream pivots from primarily encoding *what was said* to *what should come next*.

**Concrete Analysis Ideas:**

| FDG Concept | Transformer Analogue | How to Probe |
|-------------|---------------------|--------------|
| Morphosyntactic level | Token/position embeddings, early layers | Check if $x_j^0$ and low-$i$ residuals cluster by POS, morphology |
| Representational level | Mid-layer residuals | Semantic similarity probes, entity tracking |
| Interpersonal level | Pragmatic features in context | Does $T(c)$ encode speaker intent, discourse role? |
| Linguistic-function (Topic, Focus) | Attention patterns? | Do certain heads attend to topicalized elements? |
| Linguistic-operators | Features that modify interpretation | Negation, tense—where do these appear in the residual? |

**Notation Suggestions:**

To avoid confusion, we could adopt:
- **transformer-layer** / **block** for $B_i$ 
- **linguistic-level** for FDG's interpersonal/representational/morphosyntactic
- **linguistic-function** explicitly when discussing Topic, Focus, etc.

**A Provocative Hypothesis:**

The residual stream might encode something like FDG's parallel levels *in superposition*. At any position $j$, layer $i$:

$$x_j^i = x_j^{\text{morph}} + x_j^{\text{repr}} + x_j^{\text{interp}} + \ldots$$

where these components occupy different subspaces. The "logit lens" working at intermediate layers suggests morphosyntactic information (next-token prediction) is present throughout, not just at the end.

**What would validate or refute this?**

- Find directions in the residual that correlate with linguistic-level distinctions
- Check if attention heads specialize by linguistic-level
- Test whether ablating certain subspaces disrupts level-specific behaviors

Would you like me to add some of these ideas as subsections under your FDG section, or keep them as discussion points for now?

stprior: I'll track this seperately for now - I don't want to explicitly bring in linguistic concepts to the notation yet, but it should influence the structure - roughly that there are stacked layers of interpretation, bracketed start and end points with modifiers in between. I would be disappointed if can't identify multi-token words, or relate different representations of the same word, but beyond that I want the structure to emerge from the model (though in the context of analysing specific text, not from the model directly).

GitHub Copilot: That's a sound methodological stance—let FDG inform *how* you look (expecting hierarchical structure, bracketed spans, head-modifier relations) without prejudging *what* you'll find.

The key constraints you're setting:

1. **Structure should emerge from analyzing specific text**, not imposed a priori
2. **Minimum expectations**: multi-token words should be identifiable, same-word-different-form relations should be traceable
3. **Stacked interpretive layers** as a guiding intuition, but let the transformer show where the boundaries are

This suggests the analysis approach should be:
- Pick concrete examples where you expect structure (e.g., "running" vs "ran", compound words, phrases)
- Look for where attention and residual patterns *naturally* cluster or bracket
- See if the transformer's internal organization rhymes with linguistic-levels, rather than mapping them directly

The notation can stay algebraic—$T_1(c)(x)$, residual decompositions, logit projections—while the *interpretation* of what those computations mean can draw on FDG intuitions.

Shall I add a brief note to the FDG section capturing this methodological constraint? Something like "Structure should emerge from model analysis, not be imposed from linguistic theory—FDG informs expectations, not encoding."

stprior: Yes, we can update speculation based on this, and think about how the notation may actually represent hierarchical structure, bracketed spans and head-modifier relations, and how we might start looking for it. For representation we coud just use an XML like structure initially,  multiple parallel documents stacked on top of each other and aligned around positions. For analysis - thats what the rest of the document is aiming toward but not sure how we tie them together.

GitHub Copilot: Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#175-175), lines 175 to 199

Using "Replace String in File"

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#170-170), lines 170 to 199

Using "Replace String in File"


```

```
Updated the FDG section with:

1. **Methodological Constraint** — structure emerges from analysis, FDG informs expectations only; minimum bar is multi-token words and same-word relations

2. **Representing Hierarchical Structure** — XML-like stacked notation aligned to positions, marking spans, heads, and modifiers as a descriptive language

3. **Connecting Representation to Analysis** — the bridge:
   - Attention patterns → span detection
   - Residual similarity → head identification  
   - Logit projections → modifier relations

4. Two new open questions about formalizing "head of span" and multi-token word patterns

The tension you're pointing at—how do we get from the algebraic tools to the hierarchical description—is now explicit. The three proposed bridges (attention, residual similarity, logit projection) are starting hypotheses, but actually testing them on specific examples will likely refine or replace them.

Made changes.