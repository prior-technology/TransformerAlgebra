stprior: Ok, so earlier blocks can affect later blocks, not vice-versa. Can we look at the lowest block in isolation, so like T(c)(x) represents an operation where a prompted transformer acts on an embedding vector x, T_1(c)(x) represents the lowest block as an operation on x?

GitHub Copilot: Looking at your question about isolating the lowest block, let me first check the current notation and theory documentation to understand how we're representing block operations. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md#1-1), lines 1 to 100

Yes, absolutely! This is a natural way to think about it. Looking at the theory document, we have:

**Full transformer as composition of blocks:**
$$x_j^L = B_L \circ B_{L-1} \circ \cdots \circ B_1 (x_j^0)$$

So your notation $T_1(c)(x)$ for the lowest block makes sense. Here's how I'd frame it:

| Expression | Meaning |
|------------|---------|
| $T(c)(x)$ | Full prompted transformer acting on embedding $x$ |
| $T_i(c)(x)$ | Block $i$ of prompted transformer acting on residual $x$ |

**For the lowest block specifically:**

$$T_1(c)(x) = B_1(x)$$

where $B_1$ is the block operator from theory.md:
$$B_1(x) = x + \tilde{A}^1(x) + \tilde{M}^1(x + \tilde{A}^1(x))$$

**Why the lowest block is special:**

The attention in $T_1(c)$ only sees:
- The context embeddings $\{x_k^0\}_{k < j}$ (fixed once context is given)
- The input $x$ at position $j$

So $T_1(c)$ is genuinely a function just of $x$—the context provides fixed key/value vectors that don't depend on $x$.

**The composition view:**
$$T(c)(x) = T_L(c) \circ T_{L-1}(c) \circ \cdots \circ T_1(c)(x)$$

But note: for $i > 1$, $T_i(c)$ is more complex because it depends on what earlier blocks computed *at the context positions*. The causal structure means $T_1(c)$ is cleanest to analyze in isolation.

Shall I add this notation to notation.md?

stprior: I'm not yet sure if T_2(c) should represent the second block, or the first and second block combined, this may emerge as we work through examples. If we concentrate on the first block, can we break down the contribution at each position in terms of the three feature types? For example, if investigating why one token was predicted instead of another we could use these two unembedding vectors as a subspace to investigate from? We would need to look at attention calculation seperately.

GitHub Copilot: Great point about keeping the $T_i$ notation flexible for now. Let me think through the first block decomposition.

**First Block Contribution Decomposition**

For the first block acting on embedding $x$ at position $j$:

$$\Delta x_j^1 = \underbrace{\tilde{A}^1(x)}_{\text{attention}} + \underbrace{\tilde{M}^1(x + \tilde{A}^1(x))}_{\text{MLP}}$$

**Projecting onto a token subspace**

If we're investigating why token A was predicted over token B, we can define a 2D subspace spanned by $\{\overline{A}, \overline{B}\}$ and project the contributions:

$$\text{logit}(A) - \text{logit}(B) = \langle \overline{A} - \overline{B}, LN(x_j^L) \rangle$$

The "logit difference direction" $d = \overline{A} - \overline{B}$ is a single vector we can dot against any residual or contribution.

**Three feature types for block 1:**

At each position $k \leq j$, the residual $x_k^0$ entering block 1 decomposes as:

| Feature Type | Symbol | Description |
|--------------|--------|-------------|
| Token embedding | $\underline{t_k}$ | What token is at position $k$ |
| Positional embedding | $p_k$ | Where position $k$ is |
| Combined | $x_k^0 = \underline{t_k} + p_k$ | Full input to block 1 |

**Attention contribution breakdown:**

The attention output at position $j$ is:
$$\tilde{A}^1(x_j^0) = \sum_h \sum_{k \leq j} A^{1,h}_{jk} \cdot V^{1,h}(LN(x_k^0))$$

where the value computation is linear: $V^{1,h}(LN(x_k^0))$ depends on the features at position $k$.

**Separating "what" from "where":**

Since $x_k^0 = \underline{t_k} + p_k$ and the value projection is linear:
$$V^{1,h}(LN(x_k^0)) \approx V^{1,h}(LN(\underline{t_k})) + V^{1,h}(LN(p_k))$$

⚠️ **But layer norm breaks this!** $LN(a + b) \neq LN(a) + LN(b)$

So we can't cleanly separate token and position contributions through attention. We could:
1. Study empirically how much position vs token matters
2. Use the combined $x_k^0$ and accept they're entangled
3. Consider ablation experiments (zero out positional, etc.)

**For the logit difference investigation:**

We can compute:
$$\langle d, \Delta x_j^1 \rangle = \langle d, \tilde{A}^1(x_j^0) \rangle + \langle d, \tilde{M}^1(\cdot) \rangle$$

This tells us how much block 1's attention vs MLP pushed toward A vs B.

**Attention patterns separately:**

You're right that attention calculation (Q·K) is separate from value movement. The pattern $A^{1,h}_{jk}$ depends on:
- Query from position $j$: $Q^{1,h}(LN(x_j^0))$
- Keys from positions $k \leq j$: $K^{1,h}(LN(x_k^0))$

This determines *where* to look, while the value projection determines *what* gets moved.

Shall I add a section to the theory doc sketching out this "logit lens" / "logit difference" analysis framework for block 1?

stprior: I think most of this would go in speculation for now.

GitHub Copilot: Good call. Let me add this to speculation.md. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#1-1), lines 1 to 100

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#100-100), lines 100 to 136

Using "Replace String in File"


```

```
Added a new "Block-wise Analysis" section to speculation.md covering:
- $T_i(c)$ notation (leaving the composition question open)
- Why block 1 is cleanest to analyze in isolation
- Logit difference direction $d = \overline{A} - \overline{B}$ for token comparison
- The attention/MLP decomposition of $\Delta x_j^1$
- The layer norm entanglement problem for separating token vs position
- The "where" (Q·K) vs "what" (V) distinction in attention

Also added the $T_2(c)$ meaning as open question #4.

Made changes.