stprior: This summary tries in part to work through describing the inputs and outputs to a transformer model (focussing on the last position) in terms of the relationship of different vectors to each other. Can we go further in describing the components of a standard transformer model block (using Pythia as an initial reference), so rather than thinking of the MLP as a matrix multiplication we think of it as a rotation + transformation + somehing? Would it help to think of it in terms of abstract geometric algebra (i.e. a type that would work in infinite dimensional spaces)? Then dot-products of two vectors become a combination of inner and outer products, what would MLP become?

GitHub Copilot: This is a fascinating question about finding the right geometric/algebraic abstraction for transformer components. Let me first look at what's already in your documentation to understand the current framework better. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/README.md)

This gives me excellent context. Now let me address your deep question about geometric decomposition of transformer components.

## Geometric Decomposition of Transformer Components

### The MLP Layer: Beyond Matrix Multiplication

In Pythia (and most modern transformers), the MLP has structure:
$$
M(x) = W_{\text{out}} \cdot \sigma(W_{\text{in}} \cdot x + b_{\text{in}}) + b_{\text{out}}
$$

where $\sigma$ is GeLU. Let's decompose this geometrically:

| Operation | Geometric Interpretation |
|-----------|-------------------------|
| $W_{\text{in}} \cdot x$ | Projection onto $d_{\text{mlp}}$ directions (typically 4× larger than $d_{\text{model}}$) |
| $\sigma(\cdot)$ | Element-wise "gating" — selectively scales each direction |
| $W_{\text{out}} \cdot (\cdot)$ | Projects back to residual space |

**Geometrically**: The MLP decomposes the input into a set of **privileged directions** (the rows of $W_{\text{in}}$), applies a **nonlinear weighting** based on how aligned the input is with each direction, then **reassembles** using $W_{\text{out}}$.

This is closer to a **"superposition of conditional projections"** than a rotation:
$$
M(x) \approx \sum_k \sigma(\langle w^{\text{in}}_k, x \rangle) \cdot w^{\text{out}}_k
$$

Each "neuron" $k$ fires proportionally to alignment with $w^{\text{in}}_k$ and contributes $w^{\text{out}}_k$ to the output.

### Geometric Algebra Perspective

In geometric algebra (GA), we have:
- **Inner product** $a \cdot b$ — scalar, measures alignment
- **Outer (wedge) product** $a \wedge b$ — bivector, encodes the plane spanned by $a, b$
- **Geometric product** $ab = a \cdot b + a \wedge b$

For infinite-dimensional spaces (like high-$d_{\text{model}}$ residual spaces), the key insight is:

| Transformer Operation | GA Analogue |
|----------------------|-------------|
| Dot product $\langle u, v \rangle$ | Inner product (grade-0) |
| Attention pattern $\text{softmax}(QK^T)$ | Collection of inner products determining mixing coefficients |
| MLP neuron activation | Inner product with threshold/gating |

**What would MLP become in GA?**

The MLP could be written as:
$$
M(x) = \sum_k g(x \cdot d_k) \, r_k
$$

where:
- $d_k \in V$ are "detector directions" (learned)
- $r_k \in V$ are "response directions" (learned)
- $g: \mathbb{R} \to \mathbb{R}$ is the activation function

This is fundamentally a **nonlinear superposition of rank-1 maps**, where the coefficient for each rank-1 map depends on the input.

### A Proposed Abstract Decomposition

For a transformer block, we could write:

$$
B(x) = x + \underbrace{\sum_{h} \alpha_h(x) \cdot P_h(x)}_{\text{Attention: input-dependent projection}} + \underbrace{\sum_k g(x \cdot d_k) \cdot r_k}_{\text{MLP: gated superposition}}
$$

where $\alpha_h(x)$ are the attention-derived mixing weights and $P_h$ are the OV projections.

### Why Not Pure Rotation?

Rotations (orthogonal transformations) preserve norms and angles. Neither attention nor MLP does this:
- **Attention** mixes information *across positions* — more like a convex combination than rotation
- **MLP** selectively amplifies/suppresses directions — changes norm non-uniformly

The better abstraction might be **"projective geometry"** or **"cone operations"**: the MLP maps inputs to a cone of possible outputs, with the specific output depending on where in the input space you are.

### For Your Notation

You might consider notation like:

| Symbol | Meaning |
|--------|---------|
| $x \cdot d$ | Detector activation (inner product) |
| $[x]_d$ | Projection of $x$ onto direction $d$ |
| $\sum_k [x]_{d_k}^{+} r_k$ | MLP as "ReLU-gated sum" (abstracting activation) |
| $\bigvee_k d_k$ | "Feature span" — subspace spanned by detector directions |

The key insight for interpretability: **the MLP learns a discrete set of directions that are meaningful**, and the nonlinearity selects which ones to activate. This is why "neuron interpretability" sometimes works — each $(d_k, r_k)$ pair can encode a concept.

### Recommendation

For TransformerAlgebra, I'd suggest:

1. **Keep dot products as the fundamental operation** — they're the interpretable core
2. **Represent MLPs as indexed sums** over neuron-like terms, e.g., $M = \sum_k \nu_k$ where each $\nu_k: V \to V$ is a "feature detector-responder"
3. **Abstract the nonlinearity** as a gating function $g: \mathbb{R} \to \mathbb{R}^+$ applied to inner products
4. **Avoid full GA formalism** unless you need to reason about subspaces/blades explicitly — the outer product's main use would be tracking "what subspace does this operation care about?"

Would you like me to draft an extension to notation.md or a new section in Summary.ipynb exploring this geometric decomposition further?

stprior: We're not really using the notebook, lets move the contents to theory.md and speculation.md. I want to keep notation.md very concise, theory.md should cover widely understood reasoning, and in speculation.md we can work through new ideas. I may also want to keep notes based on this conversation as I try to understand the theory in tutorial.md, but we can come back to that.

GitHub Copilot: Optimizing tool selection...I'll reorganize the content from the notebook into the markdown files. Let me first read the full notebook and the existing notation.md to understand what we're working with. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/Summary.ipynb)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Now I have a clear picture. Let me create the new structure:

1. **notation.md** - Keep it concise, just the symbol definitions
2. **theory.md** - Well-established transformer theory (the Analysis and Block Structure content)
3. **speculation.md** - Exploratory ideas (Residual Space Structure, dual pairing, feature types, and our geometric algebra discussion) 

Using "Replace String in File"


```

```


Created [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md)

Created [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md)

Done! I've reorganized the content into three markdown files:

- **notation.md** — Concise symbol definitions and indexing conventions only
- **theory.md** — Well-established transformer theory (inference, residual streams, block structure, layer norm properties)
- **speculation.md** — Exploratory ideas including:
  - Dual pairing and residual space structure
  - Feature types in the residual stream
  - The geometric decomposition of MLPs we discussed
  - Geometric algebra perspectives
  - Open questions

The notebook Summary.ipynb still exists with the original content. Would you like me to delete it, or keep it as an archive? And when you're ready, we can create `tutorial.md` for your learning notes.

Made changes.

stprior: Ok, where I want to go next with this is to extend the notation to include default and prompted transformers. So if T represents a transformers default operation on an initial token, $T(t_1...t_{n})$  represents how a transformer with n specific tokens in the context window acts on the next embedding vector from the next token $x$. We can also use plaintext; T("Some context"). Can we say anything about the relation between how T("Some") and T("Some context") act on x?

GitHub Copilot: This is a great direction! Let me think through this with you.

## Prompted Transformer Notation

We're defining $T$ as the transformer's operation, where:
- $T$ alone represents the "bare" transformer
- $T(t_1, \ldots, t_n)$ or $T(\text{"Some context"})$ represents the transformer with a specific context

The key question: **What is the relationship between $T(\text{"Some"})$ and $T(\text{"Some context"})$?**

### What Changes with Context

When we extend the context from "Some" to "Some context", here's what's affected at the final position:

| Component | What changes |
|-----------|--------------|
| **Attention patterns** | New tokens can attend to all previous tokens; the final position attends to a different set |
| **Residual at final position** | Completely different—it's the embedding of "context" not "Some" |
| **Information available** | The new final position can "see" everything via attention |

### A Key Insight

For decoder-only transformers, **earlier positions are computed identically** regardless of what comes after (due to causal masking). So:

$$T(\text{"Some context"}) \text{ at position 0} = T(\text{"Some"}) \text{ at position 0}$$

The residual streams $x^i_0$ are the same in both cases. This gives us:

$$T(t_1, \ldots, t_n, t_{n+1}) = T(t_1, \ldots, t_n) + \Delta T(t_{n+1} | t_1, \ldots, t_n)$$

where $\Delta T$ captures "what the new token adds" given the existing context.

### Proposed Notation Extension

| Expression | Meaning |
|------------|---------|
| $T$ | Bare transformer (weights only) |
| $T(c)$ | Prompted transformer with context $c$ |
| $T(c)(x)$ | Action of prompted transformer on embedding $x$ at next position |
| $T(c_1, c_2)$ | Equivalent to $T(c_1 \cdot c_2)$ where $\cdot$ is concatenation |

### The Functional Relationship

If we think of $T(c)$ as a function $V \to V$ (acting on the next token's embedding), then:

$$T(c_1, c_2)(x) \neq T(c_1)(x) + T(c_2)(x)$$

Context doesn't compose additively. But we *can* say:

$$T(c_1, c_2)(x) = T(c_1) \rhd T(t_{|c_1|+1}, \ldots)(x)$$

where $\rhd$ denotes "provides context for"—the earlier context establishes residual states that later attention can read from.

### A More Precise Statement

Let $\mathcal{C} = \{x^i_j\}$ be the "context state" (all residuals at all positions for the context). Then:

$$T(c)(x) = F(x, \mathcal{C}(c))$$

where $F$ is the transformer's forward pass for the final position, which can attend to $\mathcal{C}$.

The relationship between $T(\text{"Some"})$ and $T(\text{"Some context"})$ is:
- $\mathcal{C}(\text{"Some context"})$ *extends* $\mathcal{C}(\text{"Some"})$
- The new context includes all old residuals plus new ones for "context"
- The action on $x$ differs because attention has more to attend to

---

Would you like me to add this notation to `notation.md` and develop the theory further in `theory.md` or `speculation.md`? We could also explore:
1. Whether there's a useful "derivative" $\frac{\partial T(c)}{\partial c}$ notion
2. How in-context learning relates to this framing (the prompted transformer "learns" from context)

stprior: Yes, please update the doc files based on this.

GitHub Copilot: Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#1-1), lines 1 to 50

Now I'll update all three files with the prompted transformer notation and theory: 

Using "Multi-Replace String in Files"


```

```

```

```

```

```
I've updated all three files:

**notation.md** — Added the prompted transformer symbols:
- $T$, $T(c)$, $T(t_1, \ldots, t_n)$, $T(\text{"text"})$
- $T(c)(x)$ for action on next embedding
- $\mathcal{C}(c)$ for context state

**theory.md** — Added a new section covering:
- Context state definition
- Causal structure (earlier positions unchanged by later tokens)
- Why context doesn't compose additively

**speculation.md** — Added exploratory ideas:
- Differential context $\Delta T(t_{n+1} | c)$
- Connection to in-context learning
- "Context as soft program" framing

Made changes.

stprior: Is it fair to say that the contribution to the residual stream at lower blocks in later positions cannot be affected by outputs at higher blocks in earlier positions? So $\Delta x_j^1$ can only depend on earlier tokens and weights in the first block?

GitHub Copilot: Yes, exactly right! This is a fundamental consequence of the causal structure in decoder-only transformers.

**The causal constraint is two-dimensional:**

1. **Temporal (position)**: Position $j$ cannot attend to positions $k > j$ (causal masking)
2. **Depth (layer)**: Block $i$ computes before block $i+1$ (sequential processing)

So $\Delta x_j^1$ (the contribution from block 1 at position $j$) can only depend on:
- The embeddings $x_0^0, x_1^0, \ldots, x_j^0$ (tokens at positions $\leq j$)
- The weights of block 1

It **cannot** depend on:
- Any $x_k^i$ where $k > j$ (future positions)
- Any $\Delta x_k^i$ where $i > 1$ (later blocks, at any position)

More generally:

$$\Delta x_j^i = f\left(\{x_k^{i-1}\}_{k \leq j}, \text{weights of block } i\right)$$

This creates a "light cone" structure—information can only flow forward in position and upward in layers:

```
Layer:  3    ·  ·  ·  ●  
        2    ·  ·  ●  ↑  
        1    ·  ●  ↑  ↑  
        0    ●──●──●──●
Position:    0  1  2  3
```

The residual at position 3, layer 3 can see everything in its past light cone (left and below), but nothing outside it.

Would you like me to add this causal structure analysis to theory.md? It's well-established theory and clarifies the information flow constraints nicely.

stprior: Yes please add that with references. I presume there's no equivalent of the speed-of-light, i.e. nothing stopping outputs at position 0, layer 0 affecting position 1 layer 3?

GitHub Copilot: Correct! There's no "speed limit"—information from position 0, layer 0 can reach position 1, layer 3 in a single forward pass. The attention mechanism at each layer can directly read from all earlier positions at the layer below.

In fact, this is the *point* of attention: it allows long-range information transfer without the bottleneck of sequential processing (unlike RNNs where information must traverse each position).

Let me add this to theory.md: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md#50-50), lines 50 to 90

Using "Replace String in File"


```

```
Done. I've added a new "Information Flow and Causal Structure" section that covers:
- The two constraints (causal masking + sequential computation)
- The formal dependency: $\Delta x_j^i = f(\{x_k^{i-1}\}_{k \leq j}, \theta_i)$
- The ASCII light cone diagram
- The key insight that there's no speed limit—attention enables direct long-range transfer
- References to the original transformer paper and the Anthropic circuits framework

Made changes.

stprior: One thing that confuses me is the difference between attention heads. IIRC a standard optimisation is to combine multiple attention heads into a single matrix, so how is having n heads of dimension r different to having one attention head of dimension n*r?

GitHub Copilot: Great question! The matrix combination is a computational optimization, but the heads are mathematically distinct. Let me explain:

## Why Multiple Heads ≠ One Big Head

### The Key Difference: Independent Attention Patterns

Each head computes its **own attention pattern** based on its own Q, K projections:

$$A^{i,h}_{jk} = \text{softmax}\left(\frac{(W_Q^h x_j)^T (W_K^h x_k)}{\sqrt{d_h}}\right)$$

With $n$ heads of dimension $r$:
- Each head has its own $W_Q^h, W_K^h \in \mathbb{R}^{r \times d_{\text{model}}}$
- Each head produces a **different** attention pattern $A^{i,h}$
- Head 1 might attend to "the previous noun" while head 2 attends to "the most recent verb"

With one head of dimension $n \cdot r$:
- Single $W_Q, W_K \in \mathbb{R}^{(nr) \times d_{\text{model}}}$
- Single attention pattern $A^i$—**one set of attention weights**
- All dimensions of the value vector get mixed with the same attention weights

### The Bottleneck

The attention pattern is a **bottleneck**—it's a scalar weight between each pair of positions, regardless of head dimension:

| Configuration | Attention patterns | Mixing flexibility |
|---------------|-------------------|-------------------|
| $n$ heads, dim $r$ | $n$ independent patterns | Each head can route information differently |
| 1 head, dim $nr$ | 1 pattern | All information routed the same way |

### The "Combined Matrix" Optimization

When people combine heads into one matrix, they're doing:

$$\text{MultiHead}(X) = \text{Concat}(H_1, \ldots, H_n) W_O$$

where $H_h = A^h V^h$. The concatenation and $W_O$ projection can be fused for efficiency, but **each $A^h$ is still computed separately**. The optimization is about parallelizing the computation, not changing the math.

### Intuition

Think of it this way:
- **Multi-head**: "Look at position 5 for syntax info, position 2 for semantic info, average over all for context"
- **Single head**: "Look at the same weighted combination of positions for everything"

Multi-head attention can **selectively route different types of information from different positions**, which a single head cannot do regardless of its dimension.

---

Would you like me to add this explanation to the theory or speculation docs?

stprior: I think this could go in a notebook AttentionHeads.ipynb. Could you give a toy example - what might a matrix with 3 attention heads each of dimension 2 look like as a matrix and what sort of matrix is not permitted?

GitHub Copilot: Created [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

I've created AttentionHeads.ipynb with a toy example showing:

1. **Three heads with dimension 2**, each with a different attention pattern:
   - Head 1: "Look at previous token"
   - Head 2: "Look at first token" 
   - Head 3: "Spread attention evenly"

2. **The key constraint**: At position 3, multi-head attention can produce output where:
   - Dims 0-1 come mostly from position 2
   - Dims 2-3 come mostly from position 0
   - Dims 4-5 are an average of all positions

3. **What single head cannot do**: A single 6-dimensional head must use the **same attention weights** for all dimensions. It cannot simultaneously focus dims 0-1 on position 2 while focusing dims 2-3 on position 0.

The notebook includes runnable code that demonstrates this concretely. Would you like me to run the cells so you can see the numerical output?

Made changes.

stprior: Yes please run and setup the repo so that it uses an working combination of python, numpy and other dependencies

GitHub Copilot: Configuring a Python Environment
Completed with input: {
  "resourcePath": "c:\\Users\\StephenPrior\\source\\repos\\TransformerAlgebra"
}

Installing packages ipykernel, jupyter, numpy in .venv (3.9.1)
Completed with input: {
  "packageList": [
    "numpy",
    "jupyter",
    "ipykernel"
  ],
  "resourcePath": "c:\\Users\\StephenPrior\\source\\repos\\TransformerAlgebra"
}

Ran terminal command: C:/Users/StephenPrior/source/repos/TransformerAlgebra/.venv/Scripts/python.exe -m pip install numpy jupyter ipykernel

Now let me configure the notebook and run the cells: 

Configuring notebook AttentionHeads.ipynb
Completed with input: {
  "filePath": "c:\\Users\\StephenPrior\\source\\repos\\TransformerAlgebra\\notebooks\\AttentionHeads.ipynb"
}

Retrieving Notebook summary.

There's an error in cell 2. Let me check what happened and run all code cells: 

Read output of [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#W1sZmlsZQ%3D%3D)

The kernel was started before numpy was installed. Let me restart by running the cell again: 

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#W1sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#W3sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#W5sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#W6sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X11sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X13sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X15sZmlsZQ%3D%3D)

All cells run successfully. Now let me also create a `requirements.txt` to make the setup reproducible: 

Created [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/requirements.txt)

Done! The notebook is now running with all cells executed. Here's a summary of what the toy example shows:

**Key output from position 3:**
- **Multi-head**: Different dimensions get different source mixtures
  - Dims 0-1: weights `[0, 0, 0.1, 0.9]` → focus on position 2 (previous token)
  - Dims 2-3: weights `[0.7, 0.1, 0.1, 0.1]` → focus on position 0 (first token)  
  - Dims 4-5: weights `[0.25, 0.25, 0.25, 0.25]` → uniform

- **Single head**: ALL dimensions must use the same 4 weights — impossible to simultaneously focus on position 2 for some dimensions and position 0 for others.

The difference matrix shows the expressivity gap between multi-head and single head attention.

Made changes.

stprior: Can you think of a way we could visualise this? Maybe with d_model=3 d_head=1 and show how tokens could be distributed in 3d space?

GitHub Copilot: Great idea! With `d_model=3` and 3 heads of `d_head=1`, we can visualize in 3D space. Each head controls one axis, so the output for each position can pull from different source positions along different axes. 

Ran terminal command: C:/Users/StephenPrior/source/repos/TransformerAlgebra/.venv/Scripts/python.exe -m pip install matplotlib

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Now let me run the new cells: 

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X20sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X21sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X22sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X24sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X25sZmlsZQ%3D%3D)

The visualization is complete and shows the key insight. The multi-head output point (gold star) for position 3 is **outside** the gray convex hull of single-head reachable points.

**Summary of what the 3D visualization shows:**

1. **Input vectors** (left): 4 positions in 3D space — pos0 on X-axis, pos1 on Y-axis, pos2 on Z-axis, pos3 in the middle

2. **Multi-head output** (middle): Each axis can pull from different sources:
   - X-coord: from previous token (pos 2 → low X)
   - Y-coord: from first token (pos 0 → low Y)
   - Z-coord: uniform average

3. **Single-head output** (right): All axes must use the same attention weights

4. **The proof**: The multi-head output `[0.4, 0.2, 0.38]` requires a **negative weight** (w3 = -0.05) to achieve with single-head attention — which is impossible since attention weights come from softmax (always non-negative).

**Multi-head attention can reach points outside the convex hull of inputs**, which single-head cannot.

Made changes.

stprior: Please review this to confirm accuracy. I'm starting to think that I was mistaken asking what matrices are not permitted - any matrix is permitted but what seperates the heads is how the results of attention calculations are used to weight contributions from different positions. In the last diagram there is no unreachable region, there are just unreachable combinations of positions. Am I taking this up right? If so please update the file based on a clearer understanding.

GitHub Copilot: You're absolutely right, and this is a much clearer way to think about it. Let me re-examine the key insight:

**What you're saying:**
- Any matrix multiplication is "permitted" — there's no forbidden matrix structure
- The constraint is about **how attention weights are applied**: each head computes a single set of position weights, and those weights apply uniformly to all dimensions within that head's subspace
- Multi-head lets you use **different position mixtures for different dimensions**

**The convex hull framing was misleading** because:
- The "unreachable region" framing suggests spatial constraints
- But the real constraint is: single-head must use the **same weighted combination of positions** for all output dimensions
- Multi-head can use **different weighted combinations per dimension**

Let me check the current notebook state and update it with this clearer framing: 

Retrieving Notebook summary.

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#107-107), lines 107 to 170

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#185-185), lines 185 to 340

Now I understand the full context. You're right - the "convex hull" / "unreachable region" framing is misleading. Let me update the notebook with clearer language: 

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Now let me update the convex hull visualization section to reframe it more accurately: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#335-335), lines 335 to 410

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#400-400), lines 400 to 443

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Now let me add a concluding cell that summarizes the corrected understanding: 

Edit [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb)

Now let me run the updated cells to verify everything works: 

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X24sZmlsZQ%3D%3D)

Ran [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/AttentionHeads.ipynb#X25sZmlsZQ%3D%3D)

The notebook is now updated with clearer framing. Let me also check if speculation.md needs updating: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/speculation.md#1-1), lines 1 to 50

The speculation.md looks fine - it doesn't have the misleading framing. The notebook is now updated with the correct understanding:

**Summary of changes:**

1. **Renamed the section** from "What Matrices Are Not Permitted" to "The Core Constraint: Position Mixing" — because no matrices are forbidden

2. **Reframed the key insight**: The constraint is about how attention weights combine positions across dimensions, not about forbidden matrix structures

3. **Updated the 3D visualization explanation**: Clarified that it's about "forbidden combinations of position mixtures" rather than "forbidden regions in space"

4. **Revised the final verification**: The language now emphasizes that we're looking for "one set of weights that works for ALL dimensions" — which is the actual constraint

5. **Added a summary cell** that captures the correct understanding: multi-head attention enables routing different information from different positions to different output dimensions

You're exactly right — the convex hull visualization is a *consequence* of the constraint (single attention pattern → weighted average → convex combination), but the fundamental insight is about **position mixing flexibility**, not geometric regions.

Made changes.
