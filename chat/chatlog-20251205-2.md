stprior: I want to plan out the next steps and get to the core of development. investigate.ipynb shows a basic analysis of model internal states. I would like to be able to do this as described in README.md and other docs. Instead of showing logits as weights I would want the output to show these outputs of inner products of different internal state vectors labelled with the token from that position, and showing the relevant operators and layer indexes. I would also want to be able to refer to vectors mentioned in this output and combine them in different ways. This is starting to get to where I was with previous effort around SymbolicTransformer, and as I describe it I think Julia may indeed be better suited to some of this than Python. The best approach may be to drive out clearer requirements for the other repos from here. 

GitHub Copilot: Optimizing tool selection...I'll help you plan the next steps for TransformerAlgebra. Let me first gather context from the key files to understand where you are and where you want to go. 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/README.md)

Retrieving Notebook summary.

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/notation.md)

Let me also look at the current implementation and the SymbolicTransformer Julia code to understand the patterns you've established: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/investigate.ipynb)

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/src/transformer_algebra/logit_lens.py#1-1), lines 1 to 200

Read [](file:///c%3A/Users/StephenPrior/source/repos/SymbolicTransformer/src/SymbolicTransformer.jl)

Now let me look at more of the SymbolicTransformer implementation to understand the patterns you developed there: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/SymbolicTransformer/ext/WrappedTransformer/WrappedTransformer.jl)

Read [](file:///c%3A/Users/StephenPrior/source/repos/SymbolicTransformer/notebooks/Summary.ipynb)

Read [](file:///c%3A/Users/StephenPrior/source/repos/VectorTransformer/src/VectorTransformer.jl#1-1), lines 1 to 100

Let me also check the TransformerAlgebra Summary notebook for more context on the mathematical framework: 

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/Summary.ipynb#1-1), lines 1 to 150

Read [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/theory.md#1-1), lines 1 to 80

Excellent! Now I have a comprehensive view. Let me also check the output of investigate.ipynb to see exactly what it currently produces: 

Read output of [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/investigate.ipynb#X35sZmlsZQ%3D%3D)

Read output of [](vscode-notebook-cell:/c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/notebooks/investigate.ipynb#X24sZmlsZQ%3D%3D)

Now I have a complete picture. Let me create a comprehensive plan for you. I'll write this as a markdown document that captures the requirements and direction for all three repositories. 

Created 3 todos

Created [](file:///c%3A/Users/StephenPrior/source/repos/TransformerAlgebra/doc/roadmap.md)

Starting: *Update README with next steps* (3/3)

Now let me update the README with concrete next steps: 

Using "Replace String in File"


```

```


Completed: *Update README with next steps* (3/3)

I've created a comprehensive plan. Here's the summary:

## What I Created

### roadmap.md 
A detailed planning document that covers:

1. **Current vs. Desired Output** - From raw logits like `After block 12: +786.32` to symbolic expressions like `⟨Dublin̄, x₅¹²⟩ = 0.89`

2. **Core Abstractions** - Defines `Residual`, `Transformation`, `PromptedTransformer`, and `SymbolicInnerProduct` types needed

3. **Python/Julia Division**:
   - **TransformerAlgebra (Python)**: Model interface, data extraction, Jupyter exploration
   - **SymbolicTransformer (Julia)**: Symbolic manipulation, operator overloading, LaTeX rendering
   - **VectorTransformer (Julia)**: Clean reference implementation for validation

4. **Phased Implementation**:
   - Phase 1: Extract per-block contributions, named vector registries
   - Phase 2: HDF5 export format for Python→Julia handoff
   - Phase 3: Julia symbolic types and operations

5. **API Sketches** - Concrete code examples for both languages

6. **Requirements for Other Repos** - Clear checklist of what SymbolicTransformer and VectorTransformer need

### Updated README.md
Added "Current Status" and "Immediate Next Steps" sections with actionable tasks.

## Key Insight

Your instinct is right—**Julia is better suited for the symbolic manipulation layer**. The plan proposes using Python as the "data extraction engine" (HuggingFace ecosystem is there) while Julia handles the algebraic notation with its operator overloading and macro capabilities. The HDF5 export bridge lets you work in either environment.

The first concrete step would be implementing `get_attention_contributions()` in Python to extract per-head outputs—want me to start on that?

Made changes.